{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1643881542858,
     "user": {
      "displayName": "Nelson L.",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GipnVJXi8Vo7o6qBMpdgcfZB2j9TYnsly5vDf6nig=s64",
      "userId": "03694478629413699942"
     },
     "user_tz": -480
    },
    "id": "NcSfcGFBcd4V",
    "outputId": "82c0b273-ef2c-4184-a3c6-864bfe1284b7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".CodeMirror{\n",
       "    font-size: 16px;\n",
       "    font-family: Monaco;\n",
       "}\n",
       "\n",
       "div.output_area pre {\n",
       "    font-size: 12px;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style type='text/css'>\n",
    ".CodeMirror{\n",
    "    font-size: 16px;\n",
    "    font-family: Monaco;\n",
    "}\n",
    "\n",
    "div.output_area pre {\n",
    "    font-size: 12px;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1643881542859,
     "user": {
      "displayName": "Nelson L.",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GipnVJXi8Vo7o6qBMpdgcfZB2j9TYnsly5vDf6nig=s64",
      "userId": "03694478629413699942"
     },
     "user_tz": -480
    },
    "id": "Njmo2zJTcd4a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(os.getcwd()+\"./..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 6215,
     "status": "ok",
     "timestamp": 1643881549067,
     "user": {
      "displayName": "Nelson L.",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GipnVJXi8Vo7o6qBMpdgcfZB2j9TYnsly5vDf6nig=s64",
      "userId": "03694478629413699942"
     },
     "user_tz": -480
    },
    "id": "drMVKL8eN5oS"
   },
   "outputs": [],
   "source": [
    "# import libaries\n",
    "import torch\n",
    "from torch import cuda\n",
    "from torch.utils.data import Dataset,DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class document_selection_model_config():\n",
    "    model_path = r\"./models/twin-albert-for-long-text-pair-classification/pytorch_model.bin\"\n",
    "    model_dir = r\"./models/twin-albert-for-long-text-pair-classification/\"\n",
    "    max_length = 512\n",
    "    batch_size = 12\n",
    "    doc_overlap_length = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./models/twin-albert-for-long-text-pair-classification/pytorch_model.bin'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_selection_model_config.model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AlbertModel\n",
    "\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "\n",
    "class TwinAlBerts(torch.nn.Module):\n",
    "    def __init__(self,model_config):\n",
    "        \n",
    "        super(TwinAlBerts,self).__init__()\n",
    "        \n",
    "        self.albert_layer_1 = AlbertModel.from_pretrained(model_config.model_name)\n",
    "        self.albert_layer_2 = AlbertModel.from_pretrained(model_config.model_name)\n",
    "        \n",
    "        self.pre_classifier = torch.nn.Linear(768*2,768)\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        \n",
    "        self.classifer = torch.nn.Linear(768,model_config.num_class)\n",
    "\n",
    "        self.loss_fct = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self,token_inputs_1,token_inputs_2,labels=None):\n",
    "\n",
    "        albert_outputs_1 = self.albert_layer_1(**token_inputs_1)\n",
    "        albert_outputs_2 = self.albert_layer_2(**token_inputs_2)\n",
    "        \n",
    "        pooler_output_1 = albert_outputs_1.pooler_output\n",
    "\n",
    "        pooler_output_2 = albert_outputs_2.pooler_output\n",
    "        \n",
    "        \n",
    "        concat_pooler = torch.cat([pooler_output_1,pooler_output_2],axis = 1)\n",
    "        \n",
    "        concat_pooler = self.pre_classifier(concat_pooler)\n",
    "        \n",
    "        concat_pooler = self.dropout(concat_pooler)\n",
    "        \n",
    "        logits = self.classifer(concat_pooler)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.loss_fct(logits,labels)\n",
    "                    \n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=None,\n",
    "            attentions=None,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(document_selection_model_config.model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(document_selection_model_config.model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from project_modules.utils import get_squad_v2_pandas_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_train_df, squad_dev_df = get_squad_v2_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>is_impossible</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56be85543aeaaa14008c9063</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>When did Beyonce start becoming popular?</td>\n",
       "      <td>in the late 1990s</td>\n",
       "      <td>269</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56be85543aeaaa14008c9065</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>What areas did Beyonce compete in when she was...</td>\n",
       "      <td>singing and dancing</td>\n",
       "      <td>207</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56be85543aeaaa14008c9066</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>When did Beyonce leave Destiny's Child and bec...</td>\n",
       "      <td>2003</td>\n",
       "      <td>526</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56bf6b0f3aeaaa14008c9601</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>In what city and state did Beyonce  grow up?</td>\n",
       "      <td>Houston, Texas</td>\n",
       "      <td>166</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56bf6b0f3aeaaa14008c9602</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>In which decade did Beyonce become famous?</td>\n",
       "      <td>late 1990s</td>\n",
       "      <td>276</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id    title  \\\n",
       "0  56be85543aeaaa14008c9063  Beyoncé   \n",
       "1  56be85543aeaaa14008c9065  Beyoncé   \n",
       "2  56be85543aeaaa14008c9066  Beyoncé   \n",
       "3  56bf6b0f3aeaaa14008c9601  Beyoncé   \n",
       "4  56bf6b0f3aeaaa14008c9602  Beyoncé   \n",
       "\n",
       "                                             context  \\\n",
       "0  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "1  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "2  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "3  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "4  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "\n",
       "                                            question          answer_text  \\\n",
       "0           When did Beyonce start becoming popular?    in the late 1990s   \n",
       "1  What areas did Beyonce compete in when she was...  singing and dancing   \n",
       "2  When did Beyonce leave Destiny's Child and bec...                 2003   \n",
       "3       In what city and state did Beyonce  grow up?       Houston, Texas   \n",
       "4         In which decade did Beyonce become famous?           late 1990s   \n",
       "\n",
       "   answer_start  is_impossible  \n",
       "0           269          False  \n",
       "1           207          False  \n",
       "2           526          False  \n",
       "3           166          False  \n",
       "4           276          False  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"When did Beyonce start becoming popular?\"\n",
    "context_list = squad_train_df.head(10)['context'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['context'] = context_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['question'] = question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When did Beyonce start becoming popular?</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When did Beyonce start becoming popular?</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>When did Beyonce start becoming popular?</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>When did Beyonce start becoming popular?</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When did Beyonce start becoming popular?</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   question  \\\n",
       "0  When did Beyonce start becoming popular?   \n",
       "1  When did Beyonce start becoming popular?   \n",
       "2  When did Beyonce start becoming popular?   \n",
       "3  When did Beyonce start becoming popular?   \n",
       "4  When did Beyonce start becoming popular?   \n",
       "\n",
       "                                             context  \n",
       "0  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...  \n",
       "1  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...  \n",
       "2  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...  \n",
       "3  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...  \n",
       "4  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token(question,context):\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "            text = question,\n",
    "            text_pair = context,\n",
    "            add_special_tokens = True,\n",
    "            max_length = 512,\n",
    "            padding = \"max_length\",\n",
    "            return_token_type_ids = True,\n",
    "            truncation = \"only_second\",\n",
    "#             return_tensors=   'pt'\n",
    "            )\n",
    "    \n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_token(question,context):\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        text = question,\n",
    "        text_pair = context,\n",
    "        add_special_tokens = True,\n",
    "        max_length = None,\n",
    "        padding = False,\n",
    "        return_token_type_ids = True,\n",
    "        truncation = False,\n",
    "        return_offsets_mapping = True\n",
    "        )\n",
    "    \n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_long_token(question,context,raw_token,doc_overlap_length = 32):\n",
    "    \n",
    "    first_context_end_pos = raw_token['offset_mapping'][511][1]-1# because of specical token\n",
    "    context_1 = context[:first_context_end_pos]\n",
    "    \n",
    "    sencond_char_start_pos = raw_token['offset_mapping'][511-doc_overlap_length][0]-1\n",
    "    \n",
    "    context_2 = context[sencond_char_start_pos:]\n",
    "    \n",
    "    \n",
    "    inputs_1 = get_token(question,context_1)\n",
    "    \n",
    "    inputs_2 = get_token(question,context_2)\n",
    "\n",
    "        \n",
    "    return inputs_1,inputs_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_feature(example):\n",
    "    \n",
    "    context = example['context']\n",
    "    question = example['question']\n",
    "    \n",
    "    # get raw token\n",
    "    raw_token  = get_raw_token(question,context)\n",
    "    \n",
    "    if len(raw_token['input_ids'])<=512:\n",
    "        \n",
    "        token_inputs_1,token_inputs_2 = duplicate_token(question,context)\n",
    "        \n",
    "    else:\n",
    "        token_inputs_1,token_inputs_2 = split_long_token(question,context,raw_token,doc_overlap_length = 32)\n",
    "        \n",
    "    return token_inputs_1,token_inputs_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicate_token(question,context):\n",
    "    \n",
    "    inputs = get_token(question,context)\n",
    "    \n",
    "    return inputs,inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_pairs_list = df.apply(lambda x:prepare_feature(x),axis = 1)\n",
    "tokens_left = [pair[0] for pair in token_pairs_list]\n",
    "\n",
    "tokens_left_df  = pd.DataFrame(tokens_left )\n",
    "\n",
    "tokens_right = [pair[1] for pair in token_pairs_list]\n",
    "tokens_right_df  = pd.DataFrame(tokens_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LongTextPairDataSet(Dataset):\n",
    "    \n",
    "    def __init__(self,df_pair_1,df_pair_2, label_list=None,device = \"cpu\"):\n",
    "        self.len = len(df_pair_1)\n",
    "        self.df_pair_1 = df_pair_1\n",
    "        self.df_pair_2 = df_pair_2\n",
    "        self.label_list = label_list\n",
    "        self.device = device\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        df_1 = self.df_pair_1.iloc[index]\n",
    "        df_2 = self.df_pair_2.iloc[index]\n",
    "        if self.label_list is not None:\n",
    "            labels = self.label_list[index]\n",
    "        \n",
    "        if isinstance(df_1,pd.core.series.Series):\n",
    "            pair_dict_1 = df_1.to_dict()\n",
    "            pair_dict_2 = df_2.to_dict()\n",
    "            \n",
    "        else:\n",
    "            pair_dict_1 = df_1.to_dict(orient = \"list\")\n",
    "            pair_dict_2 = df_2.to_dict(orient = \"list\")\n",
    "        \n",
    "        inputs_1 = {k:torch.tensor(v).to(self.device) for k,v in pair_dict_1.items()}\n",
    "        \n",
    "        inputs_2 = {k:torch.tensor(v).to(self.device) for k,v in pair_dict_2.items()}\n",
    "        \n",
    "        \n",
    "        if self.label_list is not None:\n",
    "            return {\"token_inputs_1\":inputs_1,\"token_inputs_2\":inputs_2,\"labels\":torch.tensor(labels).to(self.device)}\n",
    "        else:\n",
    "            return {\"token_inputs_1\":inputs_1,\"token_inputs_2\":inputs_2}\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dataset = LongTextPairDataSet(tokens_left_df,tokens_right_df,device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_inputs_1': {'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0'),\n",
       "  'input_ids': tensor([[  2,  76, 144,  ...,   0,   0,   0],\n",
       "          [  2,  76, 144,  ...,   0,   0,   0],\n",
       "          [  2,  76, 144,  ...,   0,   0,   0],\n",
       "          [  2,  76, 144,  ...,   0,   0,   0]], device='cuda:0'),\n",
       "  'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')},\n",
       " 'token_inputs_2': {'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0'),\n",
       "  'input_ids': tensor([[  2,  76, 144,  ...,   0,   0,   0],\n",
       "          [  2,  76, 144,  ...,   0,   0,   0],\n",
       "          [  2,  76, 144,  ...,   0,   0,   0],\n",
       "          [  2,  76, 144,  ...,   0,   0,   0]], device='cuda:0'),\n",
       "  'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')}}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_dataset[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pred_dataset[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-3.0678,  2.8800],\n",
       "        [-3.0678,  2.8800],\n",
       "        [-3.0678,  2.8800],\n",
       "        [-3.0678,  2.8800],\n",
       "        [-3.0678,  2.8800],\n",
       "        [-3.0678,  2.8800]], device='cuda:0'), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "SQUAD - Fine Tuned Twin-AlBerts For Long Text Pair Classification.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/huggingface/notebooks/blob/master/examples/question_answering.ipynb",
     "timestamp": 1640747938302
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
